# 🧪 A/B Test Analysis – Conversion & Revenue Comparison

This project analyzes the results of an A/B test using a real-world dataset from Kaggle. The goal is to determine whether a product variant significantly outperforms the control group in terms of user conversion and revenue.

---

## 📁 Dataset

- `AB_Test_Results.csv`  
- Fields:
  - `USER_ID`: Unique user identifier
  - `VARIANT_NAME`: Test group (`control` or `variant`)
  - `REVENUE`: Revenue generated by the user (0 if not converted)

---

## 🎯 Objective

To evaluate whether there is a statistically significant difference in:
- **Conversion Rate**
- **Average Revenue Per User**

---

## 📊 Methods Used

- **Conversion Analysis**:  
  Two-proportion Z-test to compare conversion rates between groups  
  - Control: 1.61%  
  - Variant: 1.44%  
  - **P-value**: 0.4879 → _Not significant_

- **Revenue Analysis**:  
  Independent T-test to compare revenue per user  
  - Control Avg Revenue: \$0.13  
  - Variant Avg Revenue: \$0.07  
  - **P-value**: 0.2047 → _Not significant_

---

## 📌 Conclusion

- The variant does **not** significantly outperform the control in either conversion rate or revenue.
- The control remains the better-performing option under current test results.
- Recommendation: **Do not roll out the variant** based on this data.

---

## 🛠 Tools Used

- Python (Pandas, NumPy, Seaborn)
- Scipy (`ttest_ind`)
- statsmodels (`proportions_ztest`)
- Jupyter Notebook

---

## 📂 Repository Structure

AB-Test-Analysis/
├── AB_Test_Results.csv # Dataset
├── AB_Test_Analysis.ipynb # Notebook with analysis
└── README.md # Project summary
