# ğŸ§ª A/B Test Analysis â€“ Conversion & Revenue Comparison

This project analyzes the results of an A/B test using a real-world dataset from Kaggle. The goal is to determine whether a product variant significantly outperforms the control group in terms of user conversion and revenue.

---

## ğŸ“ Dataset

- `AB_Test_Results.csv`  
- Fields:
  - `USER_ID`: Unique user identifier
  - `VARIANT_NAME`: Test group (`control` or `variant`)
  - `REVENUE`: Revenue generated by the user (0 if not converted)

---

## ğŸ¯ Objective

To evaluate whether there is a statistically significant difference in:
- **Conversion Rate**
- **Average Revenue Per User**

---

## ğŸ“Š Methods Used

- **Conversion Analysis**:  
  Two-proportion Z-test to compare conversion rates between groups  
  - Control: 1.61%  
  - Variant: 1.44%  
  - **P-value**: 0.4879 â†’ _Not significant_

- **Revenue Analysis**:  
  Independent T-test to compare revenue per user  
  - Control Avg Revenue: \$0.13  
  - Variant Avg Revenue: \$0.07  
  - **P-value**: 0.2047 â†’ _Not significant_

---

## ğŸ“Œ Conclusion

- The variant does **not** significantly outperform the control in either conversion rate or revenue.
- The control remains the better-performing option under current test results.
- Recommendation: **Do not roll out the variant** based on this data.

---

## ğŸ›  Tools Used

- Python (Pandas, NumPy, Seaborn)
- Scipy (`ttest_ind`)
- statsmodels (`proportions_ztest`)
- Jupyter Notebook

---

## ğŸ“‚ Repository Structure

AB-Test-Analysis/
â”œâ”€â”€ AB_Test_Results.csv # Dataset
â”œâ”€â”€ AB_Test_Analysis.ipynb # Notebook with analysis
â””â”€â”€ README.md # Project summary
